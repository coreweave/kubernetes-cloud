diff --git a/bloom-inference-server/models/model.py b/bloom-inference-server/models/model.py
index a16c8bb..0c8d69c 100644
--- a/bloom-inference-server/models/model.py
+++ b/bloom-inference-server/models/model.py
@@ -18,7 +18,8 @@ class Model:
         raise NotImplementedError("This is a dummy class")
 
     def generate(self, request: GenerateRequest) -> GenerateResponse:
-        input_tokens = self.tokenizer(request.text, return_tensors="pt", padding=True)
+        input_tokens = self.tokenizer(
+            request.text, return_tensors="pt", padding=True)
 
         for t in input_tokens:
             if torch.is_tensor(input_tokens[t]):
@@ -58,14 +59,17 @@ class Model:
 
         input_token_lengths = [x.shape[0] for x in input_tokens.input_ids]
         output_token_lengths = [x.shape[0] for x in output_tokens]
-        generated_tokens = [o - i for i, o in zip(input_token_lengths, output_token_lengths)]
+        generated_tokens = [o - i for i,
+                            o in zip(input_token_lengths, output_token_lengths)]
 
         if request.remove_input_from_output:
             # the generate method's output includes input too. Remove input if
             # that is requested by the user
-            output_tokens = [x[-i:] for x, i in zip(output_tokens, generated_tokens)]
+            output_tokens = [x[-i:]
+                             for x, i in zip(output_tokens, generated_tokens)]
 
-        output_text = self.tokenizer.batch_decode(output_tokens, skip_special_tokens=True)
+        output_text = self.tokenizer.batch_decode(
+            output_tokens, skip_special_tokens=True)
 
         return GenerateResponse(text=output_text, num_generated_tokens=generated_tokens)
 
@@ -79,14 +83,31 @@ class Model:
 
 
 def get_downloaded_model_path(model_name: str):
-    f = partial(
-        snapshot_download,
-        repo_id=model_name,
-        allow_patterns=["*"],
-        local_files_only=is_offline_mode(),
-        cache_dir=os.getenv("TRANSFORMERS_CACHE", None),
-    )
-    # download only on 1 process
-    run_rank_n(f, barrier=True)
-    # now since the snapshot is downloaded, pass the model_path to all processes
-    return f()
+    # Modified to not use snapshot_download() which requires write permissions.
+    # InferenceServices mount PVC's as read-only.
+    model_id_split = model_name.split('/')
+    model_org = model_id_split[0]
+    model_repo = model_id_split[1]
+
+    model_directory = (
+        "models" +
+        "--" +
+         model_org +
+         "--" +
+        model_repo
+        )
+
+    HF_HOME = os.getenv('HF_HOME', '/mnt/models')
+    HUB_CACHE = os.path.join(HF_HOME, "hub")
+    MODEL_REVISION = os.getenv('MODEL_REVISION', 'main')
+
+    model_path = os.path.join(HUB_CACHE, model_directory)
+
+    model_ref_path = os.path.join(model_path, 'refs', MODEL_REVISION)
+
+    with open(model_ref_path, 'r') as f:
+        model_git_ref = f.readlines()[0]
+
+    model_snapshot_path = os.path.join(model_path, "snapshots", model_git_ref)
+
+    return model_snapshot_path
diff --git a/bloom-inference-server/server.sh b/bloom-inference-server/server.sh
old mode 100644
new mode 100755
index 92179fb..0dcf756
--- a/bloom-inference-server/server.sh
+++ b/bloom-inference-server/server.sh
@@ -1,5 +1,5 @@
-export MODEL_NAME=bigscience/bloom
-export DEPLOYMENT_FRAMEWORK=hf_accelerate
+export MODEL_NAME=microsoft/bloom-deepspeed-inference-fp16
+export DEPLOYMENT_FRAMEWORK=ds_inference
 export DTYPE=fp16
 
 # for more information on gunicorn see https://docs.gunicorn.org/en/stable/settings.html
