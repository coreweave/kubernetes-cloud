apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: "finetune-"
spec:
  entrypoint: main
  arguments:
    parameters:
    - name: run_name
    - name: pvc
      value: 'finetune-data'
    # Training parameters.
    - name: dataset
      value: 'dataset'
    - name: model
      value: 'EleutherAI/gpt-neo-2.7B'
    - name: learn_rate
      value: '5e-5'
    - name: epochs
      value: '1'
    - name: region
      value: 'ORD1'
    - name: checkpoint
      value: 'resume'
    - name: context
      value: '2048'
    - name: tokenizer
      value: 'gpt2'
    - name: train_ratio
      value: '0.9'
    - name: batch_size
      value: '-1'
    - name: batch_size_divisor
      value: '1.0'
    - name: random_seed
      value: '42'
    - name: eot_token
      value: '<|endoftext|>'
    - name: pad_token
      value: '<|endoftext|>'
    - name: logs
      value: 'logs'
    - name: wandb_key
      value: ''
    - name: gradients
      value: '5'
    - name: zero_stage
      value: '3'
    - name: no_resume
      value: 'false'
    # Inference service configuration.
    - name: run_inference
      value: false
    - name: inference_only
      value: false
    - name: inference_image
      value: 'coreweave/ml-images:pytorch-huggingface-81d5ce11'
    #  value: 'gooseai/pytorch-huggingface:rc0'


  templates:
  - name: main
    steps:
    - - name: tokenizer
        template: model-tokenizer
        arguments:
          parameters:
          - name: input
            value: "/{{workflow.parameters.pvc}}/{{workflow.parameters.dataset}}"
          - name: output
            value: "/{{workflow.parameters.pvc}}/{{workflow.parameters.dataset}}-{{workflow.parameters.tokenizer}}-{{workflow.parameters.context}}.tokens"
          - name: tokenizer
            value: "{{workflow.parameters.tokenizer}}"
          - name: context
            value: "{{workflow.parameters.context}}"
          - name: eot
            value: "{{workflow.parameters.eot_token}}"
          - name: pad
            value: "{{workflow.parameters.pad_token}}"
          - name: boundary
            value: "\n"
        when: "{{workflow.parameters.inference_only}} == false"

    - - name: finetuner
        template: model-finetuner
        arguments:
          parameters:
          - name: run_name
            value: "{{workflow.parameters.run_name}}"
          - name: model
            value: "{{workflow.parameters.model}}"
          - name: dataset
            value: "/{{workflow.parameters.pvc}}/{{workflow.parameters.dataset}}-{{workflow.parameters.tokenizer}}-{{workflow.parameters.context}}.tokens"
          - name: learn_rate
            value: "{{workflow.parameters.learn_rate}}"
          - name: epochs
            value: "{{workflow.parameters.epochs}}"
          - name: train_ratio
            value: "{{workflow.parameters.train_ratio}}"
          - name: eot
            value: "{{workflow.parameters.eot_token}}"
          - name: pad
            value: "{{workflow.parameters.pad_token}}"
          - name: bs
            value: "{{workflow.parameters.batch_size}}"
          - name: bs_divisor
            value: "{{workflow.parameters.batch_size_divisor}}"
          - name: seed
            value: "{{workflow.parameters.random_seed}}"
          - name: output_path
            value: "/{{workflow.parameters.pvc}}/finetunes/"
          - name: cache
            value: "/{{workflow.parameters.pvc}}/cache/"
          - name: torch_cache
            value: "/{{workflow.parameters.pvc}}/torch/"
          - name: no_resume
            value: "{{workflow.parameters.no_resume}}"
          - name: logs
            value: "/{{workflow.parameters.pvc}}/{{workflow.parameters.logs}}"
          - name: wandb_key
            value: "{{workflow.parameters.wandb_key}}"
          - name: gradients
            value: "{{workflow.parameters.gradients}}"
          - name: zero_stage
            value: "{{workflow.parameters.zero_stage}}"
        when: "{{workflow.parameters.inference_only}} == false"

    - - name: inference-service
        template: model-inference-service
        arguments:
          parameters:
            - name: model_path
              value: "finetunes/results-{{workflow.parameters.run_name}}"
        when: "{{workflow.parameters.run_inference}} == true"

  - name: model-tokenizer
    inputs:
      parameters:
        - name: input
        - name: tokenizer
        - name: context
        - name: eot
        - name: pad
        - name: output
        - name: boundary
    retryStrategy:
      limit: 1
    container:
      image: ghcr.io/wbrown/gpt_bpe/dataset_tokenizer:4422662
      command: [ "/ko-app/dataset_tokenizer" ]
      args: ["-tokenizer", "{{inputs.parameters.tokenizer}}",
             "-context", "{{inputs.parameters.context}}",
             "-eot", "{{inputs.parameters.eot}}",
             "-pad", "{{inputs.parameters.pad}}",
             "-input", "{{inputs.parameters.input}}",
             "-output", "{{inputs.parameters.output}}",
             "-boundary", "{{inputs.parameters.boundary}}"]
      resources:
        requests:
          memory: 256Mi
          cpu: "4"
      volumeMounts:
        - mountPath: "/{{workflow.parameters.pvc}}"
          name: "{{workflow.parameters.pvc}}"
    volumes:
      - name: "{{workflow.parameters.pvc}}"
        persistentVolumeClaim:
           claimName: "{{workflow.parameters.pvc}}"
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/region
              operator: In
              values:
              - "{{workflow.parameters.region}}"


  - name: model-finetuner
    inputs:
      parameters:
        - name: run_name
        - name: model
        - name: dataset
        - name: learn_rate
        - name: epochs
        - name: train_ratio
        - name: eot
        - name: pad
        - name: bs
        - name: bs_divisor
        - name: seed
        - name: output_path
        - name: cache
        - name: logs
        - name: torch_cache
        - name: no_resume
        - name: wandb_key
        - name: gradients
        - name: zero_stage
    container:
      image: docker.io/gooseai/finetuner:rc39
      command: [ "/usr/bin/python3", "/app/finetuner.py" ]
      args: ["--run_name", "{{inputs.parameters.run_name}}",
             "--model", "{{inputs.parameters.model}}",
             "--dataset", "{{inputs.parameters.dataset}}",
             "--eot", "{{inputs.parameters.eot}}",
             "--pad", "{{inputs.parameters.pad}}",
             "--bs", "{{inputs.parameters.bs}}",
             "--seed", "{{inputs.parameters.seed}}",
             "--lr", "{{inputs.parameters.learn_rate}}",
             "--tr", "{{inputs.parameters.train_ratio}}",
             "--output", "{{inputs.parameters.output_path}}",
             "--cache", "{{inputs.parameters.cache}}",
             "--logs", "{{inputs.parameters.logs}}",
             "--bs_divisor", "{{inputs.parameters.bs_divisor}}",
             "--gradients", "{{inputs.parameters.gradients}}",
             "--zero_stage", "{{inputs.parameters.zero_stage}}",
             "--no_resume", "{{inputs.parameters.no_resume}}"]
      tty: true
      env:
      - name: WANDB_API_KEY
        value: "{{inputs.parameters.wandb_key}}"
      - name: PYTHONUNBUFFERED
        value: "1"
      - name: TORCH_EXTENSIONS_DIR
        value: "{{inputs.parameters.torch_cache}}"
      resources:
        requests:
          memory: 128Gi
          cpu: "8"
        limits:
          nvidia.com/gpu: 1
      volumeMounts:
        - mountPath: "/{{workflow.parameters.pvc}}"
          name: "{{workflow.parameters.pvc}}"
    volumes:
      - name: "{{workflow.parameters.pvc}}"
        persistentVolumeClaim:
           claimName: "{{workflow.parameters.pvc}}"
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: gpu.nvidia.com/vram
              operator: In
              values:
              - "48"
            - key: topology.kubernetes.io/region
              operator: In
              values:
              - "{{workflow.parameters.region}}"

  - name: model-inference-service
    inputs:
      parameters:
        - name: model_path
    resource:
      action: apply
      manifest: |
        apiVersion: serving.kubeflow.org/v1beta1
        kind: InferenceService
        metadata:
          name: inference-{{ workflow.parameters.run_name }}
          annotations:
            autoscaling.knative.dev/scaleToZeroPodRetentionPeriod: 20m
        spec:
          predictor:
            minReplicas: 0
            maxReplicas: 1
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                      - key: gpu.nvidia.com/vram
                        operator: In
                        values:
                        - "24"
                preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 10
                  preference:
                    matchExpressions:
                      - key: topology.kubernetes.io/region
                        operator: In
                        values:
                          - {{ workflow.parameters.region }}
            containers:
              - name: kfserving-container
                image: {{ workflow.parameters.inference_image }}
                imagePullPolicy: IfNotPresent
                command:
                  - "python3"
                  - "/inference/huggingface.py"
                env:
                  - name: MODEL_NAME
                    value: "final"
                  - name: MODEL_PRECISION
                    value: "fp16"
                  - name: STORAGE_URI # KFServing mounts the PVC at /mnt/pvc/
                    value: pvc://{{ workflow.parameters.pvc }}/
                  - name: MODEL_PATH
                    value: /mnt/pvc/{{ inputs.parameters.model_path }}
                resources:
                  requests:
                    nvidia.com/gpu: 1
                    cpu: 4
                    memory: 8Gi
                  limits:
                    nvidia.com/gpu: 1
                    cpu: 12
                    memory: 60Gi
