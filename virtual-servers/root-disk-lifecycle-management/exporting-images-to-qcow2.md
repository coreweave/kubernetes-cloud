# Exporting images to QCOW2

**Objective:** Create a QCOW2 image from a PVC hosted in our namespace.\
**Overview:** We will spin up a [shared filesystem](../../coreweave-kubernetes/storage.md#shared-filesystem) to store a QCOW2 image, generated by a worker pod from a mounted PVC in our namespace.

#### References:

{% file src="../../.gitbook/assets/clone_to_file.yaml" %}

{% file src="../../.gitbook/assets/shared_data.yaml" %}

## Create a shared filesystem

Creating a [shared filesystem](../../coreweave-kubernetes/storage.md#shared-filesystem) gives us a destination for our worker pod to write to, as well as a volume that can be attached to a Virtual Server or Samba Pod to egress the exported QCOW2 file.

We'll deploy the following YAML with `k create -f shared_data.yaml`.

{% tabs %}
{% tab title="YAML" %}
{% code title="shared_data.yaml" %}
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-data
spec:
  storageClassName: shared-hdd-ord1
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1000Gi
```
{% endcode %}

{% hint style="info" %}
Note we've created our shared filesystem in the **ORD** region. If our source disk exists in a different region - we'll want to change the shared data filesystem to match.
{% endhint %}
{% endtab %}
{% endtabs %}

## Identify source disk

Using `k get pvc`, we'll identify a PVC in our namespace that we wish to export:

![](<../../.gitbook/assets/image (3).png>)

{% hint style="info" %}
Note our source image exists in the **ORD** region - matching our shared data filesystem.
{% endhint %}

## Deploy worker pod

Next, we'll create a worker pod that has both our source disk, and shared data filesystem mounted.

Using `k create -f clone-to-file.yaml`:

{% tabs %}
{% tab title="YAML" %}
{% code title="clone-to-file.yaml" %}
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: clone-to-file
spec:
  parallelism: 1
  completions: 1
  template:
    metadata:
      name: clone
    spec:
      nodeSelector:
        node.coreweave.cloud/class: cpu
        ethernet.coreweave.cloud/speed: 10G
        topology.kubernetes.io/region: ORD1
        cpu.coreweave.cloud/family: epyc
      containers:
      - name: rsync
        image: ubuntu:latest
        command: ["bash", "-c", "apt update;DEBIAN_FRONTEND=noninteractive apt install -y qemu-utils; dd conv=sparse bs=4M if=/dev/xvda of=/tmp/disk.img;qemu-img convert -f raw -O qcow2 /tmp/disk.img /shared-data/disk.qcow2;rm /tmp/disk.*; echo 'Done'"]
        volumeDevices:
        - name: source
          devicePath: /dev/xvda
        volumeMounts:
        - mountPath: /shared-data
          name: shared-data
      restartPolicy: OnFailure
      volumes:
      - name: source
        persistentVolumeClaim:
          claimName: winserver2019std-clone-20210701-ord1
          readOnly: true
      - name: shared-data
        persistentVolumeClaim:
          claimName: shared-data
      tolerations:
      - key: node.coreweave.cloud/hypervisor
        operator: Exists
      - key: is_cpu_compute
        operator: Exists
```
{% endcode %}

{% hint style="warning" %}
Note that while our shared filesystem can be mounted to multiple pods/Virtual Servers simultaneously, our source block filesystem **winserver2019std-clone-20210701-ord1** must not be in use by any running pods when this job is deployed.
{% endhint %}
{% endtab %}
{% endtabs %}

Progress can be monitored with `k get pods --watch`:

![](<../../.gitbook/assets/image (2).png>)

Once the job status shows Completed, the job can be deleted with `k delete job clone-to-file`.

The shared data filesystem, with its exported QCOW2 can be attached to a Virtual Server or a [apps.coreweave.com](https://apps.coreweave.com) based File Browser or SAMBA for further inspection.
