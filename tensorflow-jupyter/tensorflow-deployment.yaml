apiVersion: apps/v1
kind: Deployment
metadata:
  name: tensorflow-jupyter
spec:
  strategy:
    type: Recreate
  # Replicas controls the number of instances of the Pod to maintain running at all times
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tensorflow-jupyter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tensorflow-jupyter
    spec:
      containers:
        - name: miner
          image: tensorflow/tensorflow:1.15.0-gpu-py3-jupyter

          ports:
            - name: notebook
              containerPort: 8888
              protocol: TCP

          readinessProbe:
            tcpSocket:
              port: notebook
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /
              port: notebook
            initialDelaySeconds: 15
            periodSeconds: 15
            failureThreshold: 3
            timeoutSeconds: 10

          volumeMounts:
            - name: storage
              mountPath: /tf/notebooks

          resources:
            requests:
              cpu: 500m # The CPU unit is mili-cores. 500m is 0.5 cores
              memory: 256Mi
            limits:
              cpu: 4000m
              memory: 2048Mi
              # GPUs can only be allocated as a limit, which both reserves and limits the number of GPUs the Pod will have access to
              # Making individual Pods resource light is advantageous for bin-packing. In the case of Jupyter, we stick to two GPUs for
              # demonstration purposes
              nvidia.com/gpu: 2

      # Node affinity can be used to require / prefer the Pods to be scheduled on a node with a specific hardware type
      # No affinity allows scheduling on all hardware types that can fulfill the resource request.
      # In this example, without affinity, any NVIDIA GPU would be allowed to run the Pod.
      # Read more about affinity at: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      affinity:
        nodeAffinity:
          # This will REQUIRE the Pod to be run on a system with a 1070 Ti, 1080 Ti or P104-100 GPU
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu.nvidia.com/model
                operator: In
                values:
                  - GeForce_GTX_1070_Ti
                  - GeForce_GTX_1080_Ti
                  - P104-100

          # As ML testing doesn't require a lot of network throughput, we try to play nice and only schedule
          # the Pod on systems with only 1G network connections. We also desire decent CPUs. This is a preference, not a requirement.
          # If systems with i5 / i9 / Xeon CPUs and/or 1G ethernet are not available to fulfill the requested resources, the Pods
          # will be scheduled on higher end systems.
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 10
              preference:
                matchExpressions:
                - key: cpu.atlantic.cloud/family
                  operator: In
                  values:
                    - i5
                    - i9
                    - xeon
            - weight: 10
              preference:
                matchExpressions:
                - key: ethernet.atlantic.cloud/speed
                  operator: In
                  values:
                    - 1G
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: jupyter-pv-claim
      restartPolicy: Always